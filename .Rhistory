abline(v = median(airquality$Wind), lty = 2, lwd = 2)
par(mfrow = c(1, 2))
plot(airquality$Wind, airquality$Ozone, main = "Ozone and Wind")
plot(airquality$Ozone, airquality$Solar.R, main = "Ozone and Solar Radiation")
par(mfrow = c(1,3), mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
plot(airquality$Wind, airquality$Ozone, main = "Ozone and Wind")
plot(airquality$Solar.R, airquality$Ozone, main = "Ozone and Solar Radiation")
plot(airquality$Temp, airquality$Ozone, main = "Ozone and Temperature")
mtext("Ozone and Weather in New York City", outer = TRUE)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv", "quiz_data.csv")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf", "quiz_codebook.pdf")
load("quiz_data.csv")
data <- read.csv("quiz_data.csv")
head(data)
dim(data)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FPUMSDataDict06.pdf", "quiz_codebook.csv")
codebook <- read.csv("quiz_codebook.csv")
codebook
getwd()
values <- data$VAL
over_1M <- subset(values, values == 24)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", "quiz_data2.xlsx")
quiz2 <- read.table("quiz_data2.xlsx")
quiz2
rows = 18:23
columns = 7:15
quiz2 <- read.xlsx("quiz_data2.xlsx", sheetIndex = 1, rowIndex = rows, colIndex = columns)
library(xlsx)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", "quiz_data2.xlsx", method = "curl")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", "quiz_data2.xlsx")
library(xlsx)
install.packages("xlsx")
library(xlsx)
library(rJava)
install.packages("openxlsx")
library(openxlsx)
install.packages("readxl")
library(readxl)
?readxl
data3 <- read_excel("quiz_data2.xlsx", sheetIndex = 1, rowIndex = rows, colIndex = columns")
[rows, columns]
?read_excel
data3 <- read_xlsx("quiz_data2.xlsx", sheet = 1, range = "G18:O23")
data3 <- read_excel("quiz_data2.xlsx", sheet = 1, range = "G18:O23")
data3 <- read_xlsx("quiz_data2.xlsx", range = "G18:O23")
data3 <- read_xlsx("quiz_data2.xlsx", range = "R18C7:R23C25")
install.packages("openxlsx")
library(openxlsx)
data3 <- read_xlsx("quiz_data2.xlsx", rows = rows, cols = columns)
data3 <- read.xlsx("quiz_data2.xlsx", rows = rows, cols = columns)
?read.xlsx
data3 <- read.xlsx("quiz_data2.xlsx", sheet = 1, rows = rows, cols = columns)
?library
download.file(url="http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx", destfile = "gas.xlsx", mode = "wb")
xx=read.xlsx("gas.xlsx",sheetIndex=1, colIndex = 7:15, rowIndex = 18:23)
library(xlsx)
library(openxlsx)
?read.xlsx
xx=read.xlsx("gas.xlsx",sheetIndex=1, cols = 7:15, rows = 18:23)
xx=read.xlsx("gas.xlsx", cols = 7:15, rows = 18:23)
xx
sum(xx$Zip*xx$Ext,na.rm=T)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", "rest.xml")
library(XML)
install.packages("XML")
library(XML)
install.packages("XML")
library(XML)
rest <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml ", useInternal = TRUE)
rest <- xmlTreeParse("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", useInternal = TRUE)
rest <- xmlTreeParse("rest.xml", useInternal = TRUE)
rest
names(rest)
rootNode <- xmlRoot(rest)
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
xmlName(rootNode)
names(rootNode)
rest <- xmlTreeParse("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml", useInternal = TRUE)
names(rest)
names(rootNode)
rootNode <- xmlRoot(rest)
names(rootNode)
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
names(rootNode)
rootNode[[1]]
rootNode[[1]][[1]]
rootNode[[1]][[1]][[1]]
rootNode[[1]][[1]][[2]]
rootNode[[1]][[2]][[2]]
length(rootNode[[1]])
length(rootNode[[1]][[1]])
xmlSApply(rootNode, xmlValue)
rootNode[[1]][[1]][[2]]
rootNode[@zipcode]
rootNode[zipcode]
/rootNode
xpathSApply(rootNode, "/zipcode", xmlValue)
test <- xpathSApply(rootNode, "/zipcode", xmlValue)
test <- xpathSApply(rootNode, "//zipcode", xmlValue)
answer <- test[test == "21231"]
download.file("http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv", "ACS.csv")
?fread
DT <- fread("ACS.csv")
library(data.table)
install.packages("data.table")
library(data.table)
?fread
DT <- fread("ACS.csv")
system.time(mean(DT$pwgtp15, by = DT$SEX))
system.time(rowMeans(DT)[DT$SEX==1]; rowMeans(DT)[DT$SEX==2])
system.time(rowMeans(DT)[DT$SEX==1])
system.time(rowMeans(DT)[DT$SEX==2])
system.time(sapply(split(DT$pwgtp15, DT$SEX), mean))
?data.table
system.time(DT[,mean(pwgtp15), by = SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))
system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(mean(DT[DT$SEX==2,]$pwgtp15); mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean))
system.time(mean(DT$pwgtp15, by = DT$SEX))
system.time(sapply(split(DT$pwgtp15, DT$SEX), mean))
ucscDb <- dbConnect(MySQL(), user = "genome", host = "genome-mysql.cse.ucsc.edu")
require(RMySQL)
source("http://biocunductor.org/biocLite.R")
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")
library(rhdf5)
created = h5createFile("example.h5")
created
created = h5createFile("example.h5", "foo")
created = h5createGroup("example.h5", "foo")
created = h5createGroup("example.h5", "baa")
created = h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
A = matrix(1:10, nr = 5, nc = 2)
B = array(seq(0.1, 2.0, by = 0.1), dim = c(5, 2, 2))
attr(B, "scale") <- "liter"
h5write(B, "example.5h", "foo/foobaa/B")
h4ls("example.h5")
h5ls("example.h5")
h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
readA = h5read("example.h5", "foo/A")
h5write(c(12, 13, 14), "example.h5", "foo/A", index = list(1:3:1))
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes = T)
library(httr)
html2 = GET(url)
content2 = content(html2, as = "text")
parsedHtml = htmlParse(content2, asText = TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
pg1 = GET("http://httpbin.org/basic-auth/user/passwd")
pg1
pg2 = GET("http://httpbin.org/basic-auth/user/passwd", authenticate("user", "passwd"))
pg2
names(pg2)
google = handle("http://google.com")
pg1 = GET(handle = google, path = "/")
pg2 = GET(handle = google, path = "search")
pg1
names(pg1)
myapp = oauth_app("twitter", key = "yourConsumerKeyHere", secret = "yourConsumerSecretHere")
sig = sign_oauth1.0(myapp, token = "yourTokenHere", token_secret = "yourTokenSecretHere")
homeTL = GET("https://api.twitter.com/1.1/statues/home_timeline.json", sig)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
library(jsonlite)
json2 = jsonlite::fromJSON(toJSON(json1))
json2
?connections
?read.foo
library(lattice)
library(datasets)
xyplot(Ozone ~ Wind, data = airquality)
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5,1))
p <- xyplot(Ozone ~ Wind, data = airquality)
print(p)
set.seed(10)
x <- rnorm(100)
f <- rep(0:1, each = 50)
y <- x + f- f * x + rnorm(100, sd = 0.5)
f <- factor(f, labels = c("Group 1", "Group 2"))
xyplot(y ~ x | f, layout = c(2,1))
xyplot(y ~ x | f, panel = function(x, y, ...) {})
xyplot(y ~ x | f, panel = function(x, y, ...) {panel.xyplot(x, y, ...); panel.lmline(x, y, col = 2)})
library(httr)
oauth_endpoints("github")
myapp <- oauth_app("github", key = "56b637a5baffac62cad9", secret = "8e107541ae1791259e9987d544ca568633da2ebf")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app("github", key = "56b637a5baffac62cad9", secret = "8e107541ae1791259e9987d544ca568633da2ebf")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app("github", key = "alphaecho1229", secret = "Ecbagjcad3679")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
myapp <- oauth_app("github", key = "af1155cb12ba046aa34e", secret = "82fd36837af02a9265a58b35877cbf98dc098a44")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
install.packages("httpuv")
library(httpuv)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
library(swirl)
swirl()
Sys.getlocale("LC_TIME")
library(lubridate)
help(package = "lubridate")
help(package = lubridate)
this_day <- today()
this_day
year(this_day)
wday(this_day)
wday(this_day, label = TRUE)
this_moment <- now()
this_moment
hour(this_moment)
my_date <- ymd("1989-05-17")
my_date
class(my_date)
ymd("1989 May 17")
mdy("March 12, 1975")
dmy(25081985)
ymd("192012")
ymd("1920/1/2")
dt1
ymd_hms(dt1)
hms("03:22"14)
hms("03:22:14")
dt2
ymd(dt2)
update(this_moment, hours = 8, minutes = 34, seconds = 55)
this_moment
this_moment <- update(this_moment, hours = 13, minutes = 27)
this_moment
?now
nyc <- now("America/New_York")
nyc
depart <- nyc + days(2)
depart
depart <- update(depart, hours = 17, minutes = 34)
depart
arrive <- depart + hours(15) + minutes(50)
?with_tz
arrive <- with_tz(arrive, "Asia/Hong_Kong")
arrive
last_time <- mdy("June 17, 2008", tz = "Singapore")
last_time
?interval
how_long <- interval(last_time, arrive)
as.period(how_long)
stopwatch()
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv", "acs.csv")
?strsplit
acs_df <- read.csv("acs.csv")
strsplit(acs.df, "wgtp")
strsplit(acs_df, "wgtp")
strsplit(names(acs_df), "wgtp")
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv", "GDP.csv")
GDP_df <- read.csv("GDP.csv")
?replace
?remove
?replace
names(GDP_df)
GDPs <- GDP_df$Gross.domestic.product.2012
head(GDPs)
head(GDP_df)
GDPs <- GDP_df$X.3
head(GDPs)
GDPs <- GDPs[5:]
GDPs <- GDPs[5:,]
GDPs <- GDPs[, 5:]
GDPs <- GDPs[5:]
GDP_df <- read.csv("GDP.csv", skip = 4)
head(GDP_df)
GDPS <- GDP_df$X.4
head(GDPS)
tail(GDPS)
GDP2 <- gsub(",", "", GDPS)
?paste
GDP2 <- past(GDPS, ",")
GDP2 <- paste(GDPS, ",")
GDP2
?strsplit
GDP2 <- strsplit(GDPS, ",", fixed = TRUE)
GDP2 <- strsplit(GDPS, ",")
class(GDPS)
GDP2 <- as.character(GDPS)
head(GDP2)
GDP2 <- str_trim(GDP2)
GDP2 <- strsplit(GDPS, ",", fixed = TRUE)
GDP2 <- strsplit(GDP2, ",", fixed = TRUE)
head(GDP2)
?paste
GDP3 <- paste(GDP2, sep = '')
head(GDP3)
GDP3[1]
GDP2
head(GDP2)
GDP2[1]
GDP3 <- paste0(GDP2)
head(GPD3)
head(GDP3)
paste0(GDP3[1])
paste0(GDP3[[1]])
library(stringr)
paste0(GDP3[[1]])
gsub(",", "", GDP2)
head(GDP_df)
head(GDP2)
head(GDPS)
GDP2 <- as.character(GDPS)
str_trim(GDP2)
GDP_df <- read.csv("GDP.csv", skip = 4, nrows = 190)
head(GDP_df)
GDPS <- GDP_df$X.4
head(GDPS)
GDP2 <- as.character(GDPS)
GDP3 <- str_trim(GDP2)
head(GDP3)
gsub(",", "", GDP3)
GDP4 <- gsub(",", "", GDP3)
mean(GDP4)
mean(as.numberic(GDP4))
mean(as.numeric(GDP4))
?grep
head(GDP_df)
countryNames <- GDP_df$X.3
grep("^United", countryNames)
download.file("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv", "education.csv")
education_df <- read.csv("education.csv")
head(education_df)
?rename
library(dplyr)
?rename
GDPS <- rename(GDPS, X = "CountryCode")
GDPS <- mutate(GDPS, CountryCode = X)
?mutate
GDPS <- mutate(GDPS, X = CountryCode)
class(GDPS$X)
class(GDPS)
GDPS <- rename(GDP_df, X = "CountryCode")
GDPS <- rename(GDP_df, CountryCode = X)
head(GDPS)
?merge
merged_df <- merge(GDPS, education_df, CountryCode)
merged_df <- merge(GDPS, education_df, "CountryCode")
head(merged_df)
names(merged_df)
notes <- merged_df$Special.Notes
grep("June|JUNE", notes, value = TRUe)
grep("June|JUNE", notes, value = TRUE)
grep("Fiscal year end: June", notes, value = TRUE)
library(quantmod)
download.package("quantmod")
download.packages("quantmod")
download.packages("quantmod", C:\Users\Alex\Documents\R\win-library\3.4)
download.packages("quantmod", Users\Alex\Documents\R\win-library\3.4)
?download.packages
install.packages("quantmod")
library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
df_2012 <- sampleTimes[grep("2012", sampleTimes)]
df_2012
df_monday <- weekdays(df_2012)
length(grep("Monday", df_monday))
swirl()
ls()
getwd()
setwd("./Coursera")
getwd()
setwd("./Coursera/Coursera_Works/Getting_and_Cleaning_Data")
setwd("./Coursera_Works/Getting_and_Cleaning_Data")
getwd()
setwd("./UCI HAR Dataset")
setwd("./Getting_and_Cleaning_Data_Project")
setwd("./UCI HAR Dataset")
file_names <- c("/activity_labels.txt", "/features.txt",
"/train/X_train.txt", "/train/y_train.txt",
"/train/subject_train.txt", "/test/X_test.txt",
"/test/y_test.txt", "/test/subject_test.txt")
file_names <- paste(cur_dir, file_names, sep = "")
cur_dir <- getwd()
file_names <- c("/activity_labels.txt", "/features.txt",
"/train/X_train.txt", "/train/y_train.txt",
"/train/subject_train.txt", "/test/X_test.txt",
"/test/y_test.txt", "/test/subject_test.txt")
file_names <- paste(cur_dir, file_names, sep = "")
features_tbl <- read.table(file_names[2], header = FALSE)
target_features <- grepl("mean[^F]|std", features_tbl$V2)
feature_names <- features_tbl[target_features, 2]
feature_names <- gsub("^t", "Time.", feature_names)
feature_names <- gsub("^f", "Frequency.", feature_names)
feature_names <- gsub("(Body)+", "Body.", feature_names)
feature_names <- gsub("Acc", "Acceleration.", feature_names)
feature_names <- gsub("Gyro", "Gryoscope.", feature_names)
feature_names <- gsub("Mag", "Magnitude.", feature_names)
feature_names <- gsub("Gravity", "Gravity.", feature_names)
feature_names <- gsub("Jerk", "Jerk.", feature_names)
feature_names <- gsub("-mean\\(\\)(-)*", "Mean.", feature_names)
feature_names <- gsub("-std\\(\\)(-)*", "Std.", feature_names)
feature_names <- c("SUBJECT.ID", feature_names, "ACTIVITY")
feature_names
X_train <- read.table(file_names[3], header = FALSE)[, target_features]
y_train <- gsub(as.character(i), activities[i], y_train)
y_train <- read.table(file_names[4], header = FALSE)
activity_labels <- read.table(file_names[1])
names(activity_labels) <- c("NUMBER", "ACTIVITY")
activities <- as.character(activity_labels$ACTIVITY)
for(i in 1:6) {
y_train <- gsub(as.character(i), activities[i], y_train)
}
length(y_train)
y_train <- read.table(file_names[4], header = FALSE)
class(y_train)
class(y_train[1])
head(y_train)
class(y_train$V1)
with(y_train, V1[V1 == 1] <- "WALKING")
head(y_train)
unique(y_train)
y_train$V1[y_train$V1 == 1] <- "WALKING"
unique(y_train)
for(i in 1:6) {
y_train$V1[y_train$V1 == i] <- activities[i]
#y_train <- gsub(as.character(i), activities[i], y_train)
}
unique(y_train)
subject_train <- read.table(file_names[5], header = FALSE)
train_merged <- cbind(subject_train, X_train, y_train)
names(train_merged)
names(train_merged) <- feature_names
names(train_merged)
X_test <- read.table(file_names[6], header = FALSE)[, target_features]
y_test <- read.table(file_names[7], header = FALSE)
for(i in 1:6) {
y_test$V1[y_test$V1 == i] <- activities[i]
}
subject_test <- read.table(file_names[8], header = FALSE)
unique(subject_test)
test_data <- cbind(subject_test, X_test, y_test)
names(test_data)
names(train_merged)
names(test_data) <- feature_names
names(test_data)
merged_data <- rbind(train_merged, test_data)
head(merged_data)
library(dplyr)
merged_data <- arrange(merged_data, SUBJECT.ID)
unique(merged_data$SUBJECT.ID)
merged_data <- rbind(train_merged, test_data)
unique(merged_data$SUBJECT.ID)
source('~/Coursera/Coursera_Works/Getting_and_Cleaning_Data/Getting_and_Cleaning_Data_Project/UCI HAR Dataset/run_analysis.R')
run_analysis()
merged_data <- group_by(merged_data, Subject.ID, Activity)
merged_data <- group_by(merged_data, SUBJECT.ID, Activity)
merged_data <- group_by(merged_data, SUBJECT.ID, ACTIVITY)
summarize(merged_data, mean)
?summarize
summarize(merged_data, mean())
summary(merged_data)
summarize(merged_data, names = mean(names))
summarize(merged_data, names = mean(names, na.rm = TRUE))
warnings()
class(merged_data)
class(merged_data$Time.Body.Acceleration.Mean.X)
without_id_activity <- feature_names[2:67]
summarize(merged_data, without_id_activity = mean(without_id_activity))
aggregate(. ~ without_id_activity, merged_data, mean)
?aggregate
aggregate(merged_data, mean)
aggregate(merged_data, FUN =mean)
?group_by
merged_data <- ungroup(merged_data)
aggregate(merged_data, by = c("SUBJECT.ID", "ACTIVITY"), FUN = mean)
aggregate(merged_data, by = ["SUBJECT.ID", "ACTIVITY"], FUN = mean)
aggregate(merged_data, by = list("SUBJECT.ID", "ACTIVITY"), FUN = mean)
?aggregate
aggregate(merged_data, by = "SUBJECT.ID", FUN = mean)
aggregate(merged_data, by = SUBJECT.ID, FUN = mean)
names(merged_data)
?summarize
grouped_data <- group_by(merged_data, SUBJECT.ID, ACTIVITY)
summarize(grouped_data, mean(na.rm = TRUE))
?summarize_each
summarize_each(grouped_data, mean(na.rm = TRUE))
summarize_each(grouped_data, mean)
summarize_all(grouped_data, mean)
summarize_each(grouped_data, "mean")
summarize_all(grouped_data, "mean")
summarize_all(grouped_data, "mean")
?write.table
source('~/Coursera/Coursera_Works/Getting_and_Cleaning_Data/Getting_and_Cleaning_Data_Project/UCI HAR Dataset/run_analysis.R')
run_analysis()
getwd()
setwd(../)
setwd(./)
?setwd
setwd("~/Coursera/Coursera_Works/Getting_and_Cleaning_Data/Getting_and_Cleaning_Data_Project")
source('~/Coursera/Coursera_Works/Getting_and_Cleaning_Data/Getting_and_Cleaning_Data_Project/UCI HAR Dataset/run_analysis.R')
source('~/Coursera/Coursera_Works/Getting_and_Cleaning_Data/Getting_and_Cleaning_Data_Project/run_analysis.R')
run_analysis()
